{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport json\nimport datetime\nimport os",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": "# The following values can be loaded from a SQL DB parameters table\n\nfilename = \"tech_assessment_transactions\"\n\n# Specify the path to your JSON file\njson_file_path = \"/drive/Snoop Tech Assessment/Raw/\"\n\narchive_file_path = f\"/drive/Snoop Tech Assessment/Archive/{datetime.date.today()}/\"\n\n# Specify list of allowed currencies\nlist_allowed_currencies = ['USD', 'GBP', 'EUR']",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "source": "### json to DataFrame",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def json_to_dataframe(filename, json_file_path, archive_file_path):\n    \"\"\"\n    Reads a JSON file, normalizes its data into a Pandas DataFrame, adds metadata columns, \n    and archives the resulting DataFrame as a CSV file.\n\n    Parameters:\n    - filename (str): The base name of the JSON file (excluding the extension).\n    - json_file_path (str): The directory path where the JSON file is located.\n    - archive_file_path (str): The directory path where the CSV archive file will be saved.\n\n    Returns:\n    pandas.DataFrame: A DataFrame containing normalized data from the JSON file, including\n    a primaryKey combining 'customerId' and 'transactionId', and an 'auditDate' column \n    indicating the current timestamp.\n\n    Example:\n    ```python\n    json_to_dataframe(\"example_data\", \"/path/to/json/files/\", \"/path/to/archive/\")\n    ```\n\n    Note:\n    - The 'transactions' key is assumed to be present in the JSON file structure.\n    - The CSV archive file is saved with the same base name as the JSON file in the specified archive directory.\n    \"\"\"\n\n    # Creating raw path including file name and extension\n    raw_full_path = json_file_path + filename + \".json\"\n    \n    # Read the JSON file directly into a dataframe\n    json_data = pd.read_json(raw_full_path)\n\n    df_normalized = pd.json_normalize(json_data['transactions'])\n\n    # Create primaryKey to identify records and check for unique values\n    df_normalized['primaryKey'] = df_normalized['customerId'] + '_' + df_normalized['transactionId']\n\n    # Adding current timestamp column\n    df_normalized['auditDate'] = pd.to_datetime(datetime.datetime.now())\n\n    # Create new path if it does not exist\n    if not os.path.exists(archive_file_path):\n        os.mkdir(archive_file_path)\n\n    # Creating archive path including file name and extension\n    archive_full_path = archive_file_path + filename + \".csv\"\n    \n    # Archive converted json data\n    df_normalized.to_csv(archive_full_path)\n    \n    return df_normalized",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "markdown",
      "source": "### Duplicate Check",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def duplicate_check(df_normalized):\n    \"\"\"\n    Performs a duplicate check on a DataFrame based on the 'primaryKey' column.\n    \n    Parameters:\n    - df_normalized (pandas.DataFrame): The DataFrame containing normalized data,\n      typically generated by the json_to_dataframe function.\n\n    Returns:\n    Tuple[str, pandas.DataFrame]: A tuple containing a message indicating the result\n    of the duplicate check and a DataFrame containing records with duplicate primary keys.\n    The message can be \"Duplicate check passed.\" if no duplicates are found, or\n    \"Duplicate check failed. Record count: <count>\" if duplicates are detected.\n    The DataFrame includes records with duplicate primary keys, selecting the\n    ones with the lower 'sourceDate' value.\n\n    Example:\n    ```python\n    message, duplicate_records = duplicate_check(df_normalized)\n    print(message)\n    print(duplicate_records)\n    ```\n\n    Note:\n    - The function assumes the presence of a 'primaryKey' column in the DataFrame.\n    - If duplicates are found, the returned DataFrame includes the selected records\n      with the lower 'sourceDate' value and an additional 'quality_check' column.\n    \"\"\"\n    \n    # Check for duplicates based on the primary key\n    duplicates = df_normalized[df_normalized.duplicated(subset='primaryKey', keep=False)]\n    \n    # If duplicates exist, select records with the lower sourceDate value\n    if not duplicates.empty:\n        # Convert SourceDate to datetime for proper comparison\n        df_normalized['sourceDate'] = pd.to_datetime(df_normalized['sourceDate'])\n        \n        # Sort by SourceDate and keep the first record for each duplicate key\n        df_duplicate_check = duplicates.sort_values(by=['primaryKey', 'sourceDate']).drop_duplicates(subset='primaryKey', keep='first')\n        df_duplicate_check['quality_check'] = 'Duplicate check'\n        \n        return_message_duplicate_check = f\"Duplicate check failed. Record count: {df_duplicate_check.count()}\"\n    else:\n        return_message_duplicate_check = \"Duplicate check passed.\"\n        df_duplicate_check = pd.DataFrame()\n\n    return return_message_duplicate_check, df_duplicate_check",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "markdown",
      "source": "### Currency Check",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def currency_check(df_normalized, list_allowed_currencies):\n    \"\"\"\n    Performs a currency check on a DataFrame, filtering out records with currencies not\n    present in the specified list of allowed currencies.\n\n    Parameters:\n    - df_normalized (pandas.DataFrame): The DataFrame containing normalized data,\n      typically generated by the json_to_dataframe function.\n    - list_allowed_currencies (list): A list of allowed currency codes.\n\n    Returns:\n    Tuple[str, pandas.DataFrame]: A tuple containing a message indicating the result\n    of the currency check and a DataFrame containing records with excluded currencies.\n    The message can be \"Currency check passed.\" if no excluded currencies are found, or\n    \"Currency check failed. Record count: <count>\" if records with excluded currencies exist.\n    The DataFrame includes records with excluded currencies and an additional 'qualityCheck'\n    column indicating the failed currency check.\n\n    Example:\n    ```python\n    message, excluded_records = currency_check(df_normalized, ['USD', 'EUR', 'GBP'])\n    print(message)\n    print(excluded_records)\n    ```\n\n    Note:\n    - The function assumes the presence of a 'currency' column in the DataFrame.\n    - The returned DataFrame includes records with currencies not present in the list of allowed currencies.\n    - The 'qualityCheck' column is added to indicate the failed currency check.\n    \"\"\"\n\n    # filter normalized \n    df_excluded_currencies = df_normalized[~df_normalized['currency'].isin(list_allowed_currencies)]\n\n    if not df_excluded_currencies.empty:\n        return_message_currency_check = f\"Currency check failed. Record count: {df_excluded_currencies.count()}\"\n        df_excluded_currencies['qualityCheck'] = 'Currency check' \n    else:\n        return_message_currency_check = \"Currency check passed.\"\n        df_excluded_currencies = pd.DataFrame()\n    return return_message_currency_check, df_excluded_currencies",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "source": "### Date check",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def date_check(df_normalized):\n    \"\"\"\n    Performs a date check on a DataFrame, verifying if the 'transactionDate' column\n    has values in the format 'yyyy-MM-dd'.\n\n    Parameters:\n    - df_normalized (pandas.DataFrame): The DataFrame containing normalized data,\n      typically generated by the json_to_dataframe function.\n\n    Returns:\n    Tuple[str, pandas.DataFrame]: A tuple containing a message indicating the result\n    of the date check and a DataFrame containing records with incorrectly formatted dates.\n    The message can be \"Date check passed.\" if all dates follow the correct format, or\n    \"Date check failed. Record count: <count>\" if records with incorrect date formats exist.\n    The DataFrame includes records with incorrectly formatted dates and an additional\n    'qualityCheck' column indicating the failed date check.\n\n    Example:\n    ```python\n    message, incorrect_date_records = date_check(df_normalized)\n    print(message)\n    print(incorrect_date_records)\n    ```\n\n    Note:\n    - The function assumes the presence of a 'transactionDate' column in the DataFrame.\n    - Dates that don't follow the correct format will be considered as Null.\n    - The 'qualityCheck' column is added to indicate the failed date check.\n    \"\"\"\n    \n    # Check if the 'Date' column has values in the format 'yyyy-MM-dd'\n    df_normalized['transactionDate_check'] = pd.to_datetime(df_normalized['transactionDate'], format='%Y-%m-%d', errors='coerce')\n\n    # dates that don't follow the correct format will be Null\n    df_date_check = df_normalized[df_normalized['transactionDate'].isnull()]\n     \n    if not df_date_check.empty:\n        return_message_date_check = f\"Date check failed. Record count: {df_date_check.count()}\"\n        df_date_check['qualityCheck'] = 'Date check'\n    else:\n        return_message_date_check = \"Date check passed.\"\n        df_date_check = pd.DataFrame()\n        \n        \n    return return_message_date_check, df_date_check",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 31
    },
    {
      "cell_type": "markdown",
      "source": "### Upsert Datamart",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def upsert_datamart(filename, datamart_filepath, df, primary_key):\n    \"\"\"\n    Upserts a DataFrame into a datamart, performing an update or insert operation based on a primary key.\n\n    Parameters:\n    - filename (str): The base name of the datamart file (excluding the extension).\n    - datamart_filepath (str): The directory path where the datamart file will be stored.\n    - df (pandas.DataFrame): The DataFrame to be upserted into the datamart.\n    - primary_key (str): The column in the DataFrame used as the primary key for the upsert operation.\n\n    Returns:\n    None\n\n    Example:\n    ```python\n    upsert_datamart(\"example_datamart\", \"/path/to/datamart/files/\", df_example, 'ID')\n    ```\n\n    Note:\n    - If the specified datamart file does not exist, a new file is created.\n    - The function uses the specified primary key column to identify and upsert records.\n    - The DataFrame is upserted into the datamart, and the updated datamart is saved to the file.\n    \"\"\"\n    \n    # Create new path if it does not exist\n    if not os.path.exists(datamart_filepath):\n        os.mkdir(datamart_filepath)\n\n    # Creating file path including file name and extension\n    datamart_full_path = datamart_filepath + filename + \".csv\"\n    \n    try:\n        # Read existing datamart from file\n        datamart = pd.read_csv(datamart_full_path, index_col=primary_key)\n    except FileNotFoundError:\n        # If the file doesn't exist, create an empty datamart\n        datamart = pd.DataFrame()\n\n    for _, row in df.iterrows():\n        key_value = row[primary_key]\n\n        if key_value in datamart.index:\n            # Update existing row\n            datamart.loc[key_value] = row\n        else:\n            # Insert new row\n            datamart = pd.concat([datamart, row.to_frame().T])\n    \n    # Save the updated datamart to the file\n    datamart.to_csv(datamart_full_path)\n    \n    print(f\"DataFrame upserted into the mock datamart at {datamart_full_path}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 47
    },
    {
      "cell_type": "markdown",
      "source": "### Error logging",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def error_logging(df1, df2, df3, error_log_path):\n    \"\"\"\n    Logs error information from three DataFrames into an error log file.\n\n    Parameters:\n    - df1, df2, df3: Pandas DataFrames representing error information.\n    - error_log_path (str): The file path to the error log file.\n\n    Returns:\n    None\n\n    Example:\n    ```python\n    error_logging(df_errors1, df_errors2, df_errors3, \"/path/to/error/log.csv\")\n    ```\n\n    Note:\n    - The function performs a union operation on the provided DataFrames and the existing error log.\n    - If the error log file doesn't exist, a new file is created.\n    - The resulting DataFrame is written to the error log file in CSV format.\n    - The 'ignore_index=True' parameter ensures a continuous index in the unioned DataFrame.\n    - The function does not handle or capture specific error information; it's designed for logging general error data.\n    \"\"\"\n    \n    # Read existing logs\n    try:\n        df_existing_log = pd.read_csv(error_log_path)\n    except FileNotFoundError:\n        # If the file doesn't exist, create an empty datamart\n        df_existing_log = pd.DataFrame()\n        \n    # Perform union operation using concat\n    unioned_df = pd.concat([df_existing_log, df1, df2, df3], ignore_index=True)\n\n    # Write the unioned DataFrame to CSV\n    unioned_df.to_csv(error_log_path, index=False)\n\n    print(f\"Unioned DataFrame written to {error_log_path}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "markdown",
      "source": "### Data Quality Checks",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Call the function to convert JSON to dataframe\ndf_normalized = json_to_dataframe(filename, json_file_path, archive_file_path)\n\n# Call the function for duplicate check\nreturn_message_duplicate_check, df_duplicate_check = duplicate_check(df_normalized)\nprint(return_message_duplicate_check)\n\n# Call the function for currency check\nreturn_message_currency_check, df_excluded_currencies = currency_check(df_normalized, list_allowed_currencies)\nprint(return_message_currency_check)\n\n# Call the function for date check\nreturn_message_date_check, df_date_check = date_check(df_normalized)\nprint(return_message_date_check)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Duplicate check failed. Record count: customerId         5\ncustomerName       5\ntransactionId      5\ntransactionDate    5\nsourceDate         5\nmerchantId         5\ncategoryId         5\ncurrency           5\namount             5\ndescription        5\nprimaryKey         5\nauditDate          5\nquality_check      5\ndtype: int64\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "<ipython-input-30-bd4651ae31f8>:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_excluded_currencies['qualityCheck'] = 'Currency check'\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Currency check failed. Record count: customerId         11\ncustomerName       11\ntransactionId      11\ntransactionDate    11\nsourceDate         11\nmerchantId         11\ncategoryId         11\ncurrency           11\namount             11\ndescription        11\nprimaryKey         11\nauditDate          11\ndtype: int64\nDate check passed.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 52
    },
    {
      "cell_type": "markdown",
      "source": "### Removing failed records",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create list of df from data quality checks\nsubset_dfs = [df_duplicate_check, df_excluded_currencies, df_date_check]\n\n# Create new df to make changes\ndf_main = df_normalized\n\n# Remove values from main DataFrame based on the 'ID' column\nfor subset_df in subset_dfs:\n    if 'primaryKey' in subset_df:\n        df_main = df_main[~df_main['primaryKey'].isin(subset_df['primaryKey'])]\n\nprint(df_main.count())",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "customerId               6848\ncustomerName             6848\ntransactionId            6848\ntransactionDate          6848\nsourceDate               6848\nmerchantId               6848\ncategoryId               6848\ncurrency                 6848\namount                   6848\ndescription              6848\nprimaryKey               6848\nauditDate                6848\ntransactionDate_check    6842\ndtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 40
    },
    {
      "cell_type": "markdown",
      "source": "### Creating df_transactions",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Creating a df of transactions\ndf_transactions = df_main[['primaryKey', 'customerId', 'transactionId', 'transactionDate', 'sourceDate', 'merchantId', 'categoryId', 'currency', 'amount', 'description', 'auditDate']]\n\nprint(df_transactions.count())",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "primaryKey         6848\ncustomerId         6848\ntransactionId      6848\ntransactionDate    6848\nsourceDate         6848\nmerchantId         6848\ncategoryId         6848\ncurrency           6848\namount             6848\ndescription        6848\nauditDate          6848\ndtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 41
    },
    {
      "cell_type": "markdown",
      "source": "### Creating df_customers",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Creating a df of customerId based on latest transactionDate\ndf_customers = df_normalized[['customerId', 'transactionDate', 'auditDate']] \\\n                            .sort_values(by='transactionDate', ascending=False) \\\n                            .drop_duplicates(subset='customerId')\n\nprint(df_customers.count())",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "customerId         29\ntransactionDate    29\nauditDate          29\ndtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 42
    },
    {
      "cell_type": "markdown",
      "source": "### Upsert df_transactions",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Set table name\ntableName = \"fact_transactions\"\n\n# Mock datamart\ndatamart_transactions_filepath = \"/drive/Snoop Tech Assessment/Datamart/fact_transactions/\"\n\n# Specify the primary key column\ntransaction_primary_key = 'primaryKey'\n\n# Upsert the df_transactions into the mock datamart\nupsert_datamart(tableName, datamart_transactions_filepath, df_transactions, transaction_primary_key)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "DataFrame upserted into the mock datamart at /drive/Snoop Tech Assessment/Datamart/fact_transactions/fact_transactions.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 48
    },
    {
      "cell_type": "markdown",
      "source": "### Upsert df_transactions",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Set table name\ntableName = \"dim_customers\"\n\n# Mock datamart\ndatamart_transactions_filepath = \"/drive/Snoop Tech Assessment/Datamart/dim_customers/\"\n\n# Specify the primary key column\ncustomers_primary_key = 'customerId'\n\n# Upsert the df_transactions into the mock datamart\nupsert_datamart(tableName, datamart_transactions_filepath, df_customers, customers_primary_key)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "DataFrame upserted into the mock datamart at /drive/Snoop Tech Assessment/Datamart/dim_customers/dim_customers.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 50
    },
    {
      "cell_type": "markdown",
      "source": "### Error logging",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "error_log_path = \"/drive/Snoop Tech Assessment/Datamart/error_logs/error_logs.csv\"\n\nerror_logging(df_duplicate_check, df_excluded_currencies, df_date_check, error_log_path)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Unioned DataFrame written to /drive/Snoop Tech Assessment/Datamart/error_logs/error_logs.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 51
    }
  ]
}